this project gets data from the wikipideas data dump of documents. first 500 records were used due to computational constraints

Map Reduce is used for implementing TF/IDF vector space model. indexer calculates the tf/idf for each term and ots output is input for query function which gives out relevent document based on similarity.